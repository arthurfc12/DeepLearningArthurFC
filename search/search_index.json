{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#template-de-entrega","title":"Template de Entrega","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#grupokit-x","title":"Grupo/Kit X","text":"<ol> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> <li>Maria Oliveira</li> <li>Grupo K<ul> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> </ul> </li> </ol> <p>Instru\u00e7\u00f5es</p> <p>Voc\u00eas devem utilizar este template como um bloco de notas para registrar o que foi feito e o que falta fazer. Voc\u00eas devem adicionar as informa\u00e7\u00f5es necess\u00e1rias. O template deve ser editado e atualizado a cada entrega, registrando assim a data de entrega e o que foi feito at\u00e9 o momento via Git.</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#diagramas","title":"Diagramas","text":"<p>Use o Mermaid para criar os diagramas de documenta\u00e7\u00e3o.</p> <p>Mermaid Live Editor</p> <pre><code>flowchart TD\n    Deployment:::orange --&gt;|defines| ReplicaSet\n    ReplicaSet --&gt;|manages| pod((Pod))\n    pod:::red --&gt;|runs| Container\n    Deployment --&gt;|scales| pod\n    Deployment --&gt;|updates| pod\n\n    Service:::orange --&gt;|exposes| pod\n\n    subgraph  \n        ConfigMap:::orange\n        Secret:::orange\n    end\n\n    ConfigMap --&gt; Deployment\n    Secret --&gt; Deployment\n    classDef red fill:#f55\n    classDef orange fill:#ffa500</code></pre>"},{"location":"#codigos","title":"C\u00f3digos","text":"De um arquivo remotoAnota\u00e7\u00f5es no c\u00f3digo main.yaml<pre><code>name: ci\non:\n  - push\n  - pull_request\n\n# Environment\nenv:\n  CI: true\n  PYTHON_VERSION: 3.12\n\n# Jobs to run\njobs:\n\n  # Build and deploy documentation site\n  deploy:\n    if: github.event_name != 'pull_request' &amp;&amp; github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n\n      # Checkout source form GitHub\n      - uses: actions/checkout@v4\n\n      # Install Python runtime and dependencies\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      # pip\n      - run: |\n          pip install -r requirements.txt\n\n      # deploy\n      - run: |\n          mkdocs gh-deploy --force\n</code></pre> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol>"},{"location":"#exemplo-de-video","title":"Exemplo de v\u00eddeo","text":"<p>Lorem ipsum dolor sit amet</p>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"projeto/main/","title":"Projeto","text":""},{"location":"projeto/main/#spotify-classification-project","title":"Spotify Classification project","text":""},{"location":"projeto/main/#author-arthur-carvalho","title":"Author - Arthur Carvalho","text":"<p>In this project, you will tackle a real-world classification task using a Multi-Layer Perceptron (MLP) neural network. The goal is to deepen your understanding of neural networks by handling data preparation, model implementation, training strategies, and evaluation without relying on high-level deep learning libraries. You will select a public dataset suitable for classification, process it, build and train your MLP, and analyze the results.</p>"},{"location":"projeto/main/#dataset-selection","title":"Dataset Selection","text":"<p>Choose a public dataset for a classification problem. Sources include:</p> <ul> <li>Kaggle (e.g., datasets for digit recognition, spam detection, or medical diagnosis).</li> <li>UCI Machine Learning Repository (e.g., Banknote Authentication, Adult Income, or Covertype).</li> <li>Other open sources like OpenML, Google Dataset Search, or government data portals (e.g., data.gov).</li> <li>Also, consider datasets from LOTS, here you have direct access to business problems.</li> <li> <p>Ensure the dataset has at least 1,000 samples and multiple features (at least 5) to make the MLP meaningful.</p> </li> <li> <p>If selecting from a competition platform, note the competition rules and ensure your work complies.</p> </li> <li>In your report: Provide the dataset name, source URL, size (rows/columns), and why you chose it (e.g., relevance to real-world problems, complexity).</li> </ul>"},{"location":"projeto/main/#dataset-explanation","title":"Dataset Explanation","text":"<ul> <li>Describe the dataset in detail: What does it represent? What are the features (inputs) and their types (numerical, categorical)? What is the target variable (classes/labels)?</li> <li>Discuss any domain knowledge: E.g., if it's a medical dataset, explain key terms.</li> <li>Identify potential issues: Imbalanced classes, missing values, outliers, or noise.</li> <li>In your report: Include summary statistics (e.g., mean, std dev, class distribution) and visualizations (e.g., histograms, correlation matrices).</li> </ul>"},{"location":"projeto/main/#data-cleaning-and-normalization","title":"Data Cleaning and Normalization","text":"<ul> <li>Clean the data: Handle missing values (impute or remove), remove duplicates, detect and treat outliers.</li> <li>Preprocess: Encode categorical variables (e.g., one-hot encoding), normalize/scale numerical features (e.g., min-max scaling or z-score standardization).</li> <li>You may use libraries like Pandas for loading/cleaning and SciPy/NumPy for normalization.</li> <li>In your report: Explain each step, justify choices (e.g., \"I used median imputation for missing values to avoid skew from outliers\"), and show before/after examples (e.g., via tables or plots).</li> </ul>"},{"location":"projeto/main/#mlp-implementation","title":"MLP Implementation","text":"<ul> <li>Code an MLP from scratch using only NumPy (or equivalent) for operations like matrix multiplication, activation functions, and gradients.</li> <li>Architecture: At minimum, include an input layer, one hidden layer, and output layer. Experiment with more layers/nodes for better performance.</li> <li>Activation functions: Use sigmoid, ReLU, or tanh.</li> <li>Loss function: Cross-entropy for classification.</li> <li>Optimizer: Stochastic Gradient Descent (SGD) or a variant like mini-batch GD.</li> <li>Pre-built neural network libraries allowed, but you must understand and explain all parts of the code and analysis submitted.</li> <li>In your report: Provide code or key code snippets (the full code). Explain hyperparameters (e.g., learning rate, number of epochs, hidden units).</li> </ul>"},{"location":"projeto/main/#model-training","title":"Model Training","text":"<ul> <li>Train your MLP on the prepared data.</li> <li>Implement the training loop: Forward propagation, loss calculation, backpropagation, and parameter updates.</li> <li>Handle initialization (e.g., random weights) and regularization if needed (e.g., L2 penalty, but optional).</li> <li>In your report: Describe the training process, including any challenges (e.g., vanishing gradients) and how you addressed them.</li> </ul>"},{"location":"projeto/main/#training-and-testing-strategy","title":"Training and Testing Strategy","text":"<ul> <li>Split the data: Use train/validation/test sets (e.g., 70/15/15 split) or k-fold cross-validation.</li> <li>Training mode: Choose batch, mini-batch, or online (stochastic) training; explain why (e.g., \"Mini-batch for balance between speed and stability\").</li> <li>Early stopping or other techniques to prevent overfitting.</li> <li>In your report: Detail the split ratios, random seeds for reproducibility, and rationale. Discuss validation's role in hyperparameter tuning.</li> </ul>"},{"location":"projeto/main/#error-curves-and-visualization","title":"Error Curves and Visualization","text":"<ul> <li>Plot training and validation loss/accuracy curves over epochs.</li> <li>Use Matplotlib or similar for plots.</li> <li>Analyze: Discuss convergence, overfitting/underfitting, and adjustments made.</li> <li>In your report: Include at least two plots (e.g., loss vs. epochs, accuracy vs. epochs). Interpret trends (e.g., \"Loss plateaus after 50 epochs, indicating convergence\").</li> </ul>"},{"location":"projeto/main/#evaluation-metrics","title":"Evaluation Metrics","text":"<ul> <li>Apply classification metrics on the test set: Accuracy, precision, recall, F1-score, confusion matrix (for multi-class).</li> <li>If imbalanced, include ROC-AUC or precision-recall curves.</li> <li>Compare to baselines (e.g., majority class predictor).</li> <li>In your report: Present results in tables (e.g., metric values) and visualizations (e.g., confusion matrix heatmap). Discuss strengths/weaknesses (e.g., \"High recall on class A but low on B due to imbalance\").</li> </ul>"},{"location":"projeto2/main/","title":"Projeto 2","text":""},{"location":"projeto2/main/#2-regression","title":"2 - Regression","text":""},{"location":"projeto2/main/#author-arthur-carvalho","title":"Author - Arthur Carvalho","text":"<p>In this project, you will tackle a real-world regression task using a Multi-Layer Perceptron (MLP) neural network. The goal is to deepen your understanding of neural networks by handling data preparation, model implementation, training strategies, and evaluation without relying on high-level deep learning libraries. You will select a public dataset suitable for regression, process it, build and train your MLP, and analyze the results.</p>"},{"location":"projeto2/main/#dataset-selection","title":"Dataset Selection","text":"<p>Choose a public dataset for a regression problem. Sources include:</p> <ul> <li>Kaggle (e.g., datasets for house price prediction, stock prices, or energy consumption).</li> <li>UCI Machine Learning Repository (e.g., Concrete Compressive Strength, Air Quality, or Wine Quality).</li> <li>Other open sources like OpenML, Google Dataset Search, or government data portals (e.g., data.gov).</li> <li>Also, consider datasets from LOTS, here you have direct access to business problems.</li> <li> <p>Ensure the dataset has at least 1,000 samples and multiple features (at least 5) to make the MLP meaningful.</p> </li> <li> <p>If selecting from a competition platform, note the competition rules and ensure your work complies.</p> </li> <li>In your report: Provide the dataset name, source URL, size (rows/columns), and why you chose it (e.g., relevance to real-world problems, complexity).</li> </ul>"},{"location":"projeto2/main/#dataset-explanation","title":"Dataset Explanation","text":"<p>Describe the dataset in detail: What does it represent? What are the features (inputs) and their types (numerical, categorical)? What is the target variable (continuous value)? Discuss any domain knowledge: E.g., if it's a financial dataset, explain key terms. Identify potential issues: Missing values, outliers, or noise. In your report: Include summary statistics (e.g., mean, std dev) and visualizations (e.g., histograms, correlation matrices).</p>"},{"location":"projeto2/main/#data-cleaning-and-normalization","title":"Data Cleaning and Normalization","text":"<p>Clean the data: Handle missing values (impute or remove), remove duplicates, detect and treat outliers. Preprocess: Encode categorical variables (e.g., one-hot encoding), normalize/scale numerical features (e.g., min-max scaling or z-score standardization). You may use libraries like Pandas for loading/cleaning and SciPy/NumPy for normalization. In your report: Explain each step, justify choices (e.g., \"I used median imputation for missing values to avoid skew from outliers\"), and show before/after examples (e.g., via tables or plots).</p>"},{"location":"projeto2/main/#mlp-implementation","title":"MLP Implementation","text":"<p>Code an MLP from scratch using only NumPy (or equivalent) for operations like matrix multiplication, activation functions, and gradients. Architecture: At minimum, include an input layer, oneW hidden layer, and output layer. Experiment with more layers/nodes for better performance. Activation functions: Use sigmoid, ReLU, or tanh. Loss function: Cross-entropy for classification. Optimizer: Stochastic Gradient Descent (SGD) or a variant like mini-batch GD. Pre-built neural network libraries allowed, but you must understand and explain all parts of the code and analysis submitted. In your report: Provide code or key code snippets (the full code). Explain hyperparameters (e.g., learning rate, number of epochs, hidden units).</p>"},{"location":"projeto2/main/#model-training","title":"Model Training","text":"<p>Train your MLP on the prepared data. Implement the training loop: Forward propagation, loss calculation, backpropagation, and parameter updates. Handle initialization (e.g., random weights) and regularization if needed (e.g., L2 penalty, but optional). In your report: Describe the training process, including any challenges (e.g., vanishing gradients) and how you addressed them.</p>"},{"location":"projeto2/main/#training-and-testing-strategy","title":"Training and Testing Strategy","text":"<p>Split the data: Use train/validation/test sets (e.g., 70/15/15 split) or k-fold cross-validation. Training mode: Choose batch, mini-batch, or online (stochastic) training; explain why (e.g., \"Mini-batch for balance between speed and stability\"). Early stopping or other techniques to prevent overfitting. In your report: Detail the split ratios, random seeds for reproducibility, and rationale. Discuss validation's role in hyperparameter tuning.</p>"},{"location":"projeto2/main/#error-curves-and-visualization","title":"Error Curves and Visualization","text":"<p>Plot training and validation loss/accuracy curves over epochs. Use Matplotlib or similar for plots. Analyze: Discuss convergence, overfitting/underfitting, and adjustments made. In your report: Include at least two plots (e.g., loss vs. epochs, accuracy vs. epochs). Interpret trends (e.g., \"Loss plateaus after 50 epochs, indicating convergence\").</p>"},{"location":"projeto2/main/#evaluation-metrics","title":"Evaluation Metrics","text":"<ul> <li>Apply regression metrics suitable for regression tasks, such as Mean Absolute Error (MAE), MAPE, Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared (R\u00b2).</li> <li>Compare to baselines (e.g., mean predictor).</li> <li>In your report: Present results in tables (e.g., metric values) and visualizations (e.g., residual plots). Discuss strengths/weaknesses (e.g., \"High RMSE indicates model struggles with outliers\").</li> </ul>"},{"location":"roteiro1/main/","title":"Roteiro 1","text":""},{"location":"roteiro1/main/#1-data-preparation-and-analysis-for-neural-networks","title":"1 - Data Preparation and Analysis for Neural Networks","text":"<p>This activity is designed to test your skills in generating synthetic datasets, handling real-world data challenges, and preparing data to be fed into neural networks.</p>"},{"location":"roteiro1/main/#excercise-1-exploring-class-separability-in-2d","title":"Excercise 1 - Exploring Class Separability in 2D","text":"<p>generate and visualize a two-dimensional dataset to explore how data distribution affects the complexity of the decision boundaries a neural network would need to learn.</p>"},{"location":"roteiro1/main/#generate-the-data","title":"Generate the Data","text":"<p>Start by importing necessary libraries for this project:</p> pip install matplotlib pandas scikit-learn numpy <p>Create a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each). Use a Gaussian distribution to generate the points for each class based on the following parameters:</p> <ul> <li>Class 0: Mean = [2,3] , Standard Deviation = [0.8,2.5]</li> <li>Class 1: Mean = [5,6], Standard Deviation = [1.2,1.9]</li> <li>Class 2: Mean = [8,1], Standard Deviation = [0.9,0.9]</li> <li>Class 3: Mean = [15,4], Standard Deviation = [0.5,2.0]</li> </ul> <p>To do this, we run the code below, which creates the points and the distribution using a random state:</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(42)\n\nparams = {\n    0: {\"mean\": np.array([2.0, 3.0]), \"std\": np.array([0.8, 2.5])},\n    1: {\"mean\": np.array([5.0, 6.0]), \"std\": np.array([1.2, 1.9])},\n    2: {\"mean\": np.array([8.0, 1.0] ), \"std\": np.array([0.9, 0.9])},\n    3: {\"mean\": np.array([15.0, 4.0]), \"std\": np.array([0.5, 2.0])},\n}\n\nn_samples_class = 100\n\ndata_list = []\nfor cls, p in params.items():\n    samples = rng.normal(loc=p[\"mean\"], scale=p[\"std\"], size=(n_samples_class, 2))\n    labels = np.full((n_samples_class, 1), cls, dtype=int)\n    data_list.append(np.hstack([samples, labels]))\n\n# Combine\ndata = np.vstack(data_list)\ndf = pd.DataFrame(data, columns=[\"x1\", \"x2\", \"label\"]).astype({\"label\": int})\n\n# Randomize rows -&gt; use seed for random state\ndf = df.sample(frac=1.0, random_state=123).reset_index(drop=True)\ncsv_path = \"synthetic_gaussian_4class_400.csv\"\ndf.to_csv(csv_path, index=False)\n\ncounts = df[\"label\"].value_counts().sort_index()\ndf.head(500)\n</code></pre>"},{"location":"roteiro1/main/#plot-the-data","title":"Plot the data","text":"<p>By plotting the data created with the above code, we get the following output:</p> <p></p> <p>Scatter plot of points</p> <p>The following can be seen in the plot:</p>"},{"location":"roteiro1/main/#class-0-mean-23-elongated-vertically-slight-spread-horizontally","title":"Class 0 (mean [2,3], elongated vertically, slight spread horizontally)","text":"<ul> <li>Class 0 is spread more in the vertical direction. It also has a slight spread horizontally.</li> <li>The minimum and maximum values of the points in Class 0 are:</li> <li>min_x: 0.294363</li> <li>max_x: 3.713318</li> <li>min_y: -1.318301</li> <li>max_y: 10.284656</li> <li>As such, it's points form a tall, narrow cluster near the lower-left corner.</li> </ul>"},{"location":"roteiro1/main/#class-1-mean-56-larger-spread-horizontally-and-vertically","title":"Class 1 (mean [5,6], larger spread horizontally and vertically)","text":"<ul> <li>Centered above Class 0 and class 2, to the left of Class 0 and to the right of Class 2, it has the largest horizontal spread.</li> <li>The minimum and maximum values of the points in Class 1 are:</li> <li>min_x: 2.227876</li> <li>max_x: 8.486081</li> <li>min_y: 1.123349</li> <li>max_y: 10.422541</li> <li>Its cluster overlaps somewhat in vertical extent with Class 0.</li> <li>Does not appear to have a distinguished shape</li> </ul>"},{"location":"roteiro1/main/#class-2-mean-81-compact-round-cluster","title":"Class 2 (mean [8,1], compact \"round\" cluster)","text":"<ul> <li>This class is tighter (std = [0.9, 0.9]) and appears in the bottom-middle region.</li> <li>Class 2 doesn\u2019t overlap much with Classes 0 or 1 because it\u2019s horizontally farther away and has limited vertical amplitude.</li> <li>The minimum and maximum values of the points in Class 1 are:</li> <li>min_x: 5.331924</li> <li>max_x: 10.178374</li> <li>min_y: -1.100255</li> <li>max_y: 3.337906</li> <li>Limited amplitudes horizontally and vertically create \"rounded\" appearance of cluster</li> </ul>"},{"location":"roteiro1/main/#class-3-mean-154-elongated-horizontally","title":"Class 3 (mean [15,4], elongated horizontally)","text":"<ul> <li>Very far to the right, centered at x \u2248 15, with std = [0.5, 2.0].</li> <li>Does not overlap with any of the other classes.</li> <li>As such, it forms a tall, narrow vertical band, to the far right of the plot.</li> <li>The minimum and maximum values of the points in Class 1 are:</li> <li>min_x: 13.839822</li> <li>max_x: 16.191301</li> <li>min_y: -1.129489</li> <li>max_y: 9.098656</li> </ul> <p>Because some overlap is seen between classes, a linear boundary will not separate each class perfectly, producing a few outliers in the process. With that being said, each Class significantly occupies a unique region in the graph. As such, disregarding outliers, a designating a linear boundary is possible</p>"},{"location":"roteiro1/main/#analyze-and-draw-boundaries","title":"Analyze and Draw Boundaries","text":"<p>With that, we can plot the scatter plot with boundaries. The code to do so is as such:</p> <pre><code>min_x_0 = bounds.loc[bounds[\"label\"] == 0, \"min_x\"].values[0] + params[0][\"std\"][0]\nmin_x_1 = bounds.loc[bounds[\"label\"] == 1, \"min_x\"].values[0] + params[1][\"std\"][0]\nmin_x_2 = bounds.loc[bounds[\"label\"] == 2, \"min_x\"].values[0] + 5*params[2][\"std\"][0] # std has lower impact, increase size of relevance\nmin_x_3 = bounds.loc[bounds[\"label\"] == 3, \"min_x\"].values[0] - 5*params[3][\"std\"][0] # std has lower impact, increase size of relevance\n\n\nmax_x_0 = bounds.loc[bounds[\"label\"] == 0, \"max_x\"].values[0] + params[0][\"std\"][0]\nmax_x_1 = bounds.loc[bounds[\"label\"] == 1, \"max_x\"].values[0] + params[1][\"std\"][0]\nmax_x_2 = bounds.loc[bounds[\"label\"] == 2, \"max_x\"].values[0] + params[2][\"std\"][0]\nmax_x_3 = bounds.loc[bounds[\"label\"] == 3, \"max_x\"].values[0] - 2*params[3][\"std\"][0] # std has lower impact, increase size of relevance\n\n\nmin_y_0 = bounds.loc[bounds[\"label\"] == 0, \"min_y\"].values[0] - params[0][\"std\"][1]\nmin_y_1 = bounds.loc[bounds[\"label\"] == 1, \"min_y\"].values[0] - params[1][\"std\"][1]\nmin_y_2 = bounds.loc[bounds[\"label\"] == 2, \"min_y\"].values[0] - params[2][\"std\"][1]\nmin_y_3 = bounds.loc[bounds[\"label\"] == 3, \"min_y\"].values[0] - params[3][\"std\"][1]\n\n\nmax_y_0 = bounds.loc[bounds[\"label\"] == 0, \"max_y\"].values[0] + params[0][\"std\"][1]\nmax_y_1 = bounds.loc[bounds[\"label\"] == 1, \"max_y\"].values[0] - 2*params[1][\"std\"][1] # std has lower impact, increase size of relevance\nmax_y_2 = bounds.loc[bounds[\"label\"] == 2, \"max_y\"].values[0] + 5*params[2][\"std\"][1] # std has lower impact, increase size of relevance\nmax_y_3 = bounds.loc[bounds[\"label\"] == 3, \"max_y\"].values[0] + params[3][\"std\"][1]\n\nstds = df.groupby(\"label\").agg(std_x=(\"x1\",\"std\"), std_y=(\"x2\",\"std\")).reset_index()\n\nbounds_std = bounds.merge(stds, on=\"label\")\n\nplt.figure(figsize=(7,5))\nfor cls in sorted(df[\"label\"].unique()):\n    subset = df[df[\"label\"] == cls]\n    plt.scatter(subset[\"x1\"], subset[\"x2\"], label=f\"class {cls}\", s=10)\n\nplt.plot([min_x_0, max_x_0], [max_y_0, min_y_0], linewidth=2)\nplt.plot([min_x_1, max_x_1], [min_y_1, max_y_1], linewidth=2)\nplt.plot([min_x_2, max_x_2], [max_y_2, min_y_2], linewidth=2)\nplt.plot([min_x_3, max_x_3], [min_y_3, max_y_3], linewidth=2)\n\nplt.title(\"Scatter Plot with Diagonal Lines per Class\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.show()\n</code></pre> <p>With that, the expected output is as such:</p> <p></p> <p>Scatter plot of points with boundaries</p>"},{"location":"roteiro1/main/#excercise-2-non-linearity-in-higher-dimensions","title":"Excercise 2 - Non-Linearity in Higher Dimensions","text":"<p>Deep networks excel when data is not linearly separable. This exercise challenges you to create and visualize such a dataset.</p>"},{"location":"roteiro1/main/#generate-the-data_1","title":"Generate the data","text":"<pre><code># Parameters\nmean_A = np.array([0, 0, 0, 0, 0], dtype=float)\ncov_A = np.array([\n    [1,   0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0],\n], dtype=float)\n\nmean_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5], dtype=float)\ncov_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5],\n], dtype=float)\n\nn_A, n_B = 500, 500\n\n# Helper: robust multivariate normal sampler\ndef sample_mvn(mean, cov, n, rng, max_tries=8):\n    d = mean.shape[0]\n    eps = 1e-10\n    cov_adj = cov.copy()\n    for _ in range(max_tries):\n        try:\n            L = np.linalg.cholesky(cov_adj)\n            Z = rng.normal(size=(n, d))\n            return mean + Z @ L.T\n        except np.linalg.LinAlgError:\n            cov_adj = cov_adj + eps * np.eye(d)\n            eps *= 10.0\n    # final fallback: eigenvalue clipping\n    w, V = np.linalg.eigh(cov)\n    w = np.clip(w, 1e-8, None)\n    cov_psd = (V * w) @ V.T\n    L = np.linalg.cholesky(cov_psd)\n    Z = rng.normal(size=(n, d))\n    return mean + Z @ L.T\n\n# Generate samples\nXA = sample_mvn(mean_A, cov_A, n_A, rng)\nXB = sample_mvn(mean_B, cov_B, n_B, rng)\n\n# Build DataFrame\ncols = [f\"f{i}\" for i in range(1, 6)]\ndf_A = pd.DataFrame(XA, columns=cols); df_A[\"label\"] = \"A\"\ndf_B = pd.DataFrame(XB, columns=cols); df_B[\"label\"] = \"B\"\n\ndf = pd.concat([df_A, df_B], ignore_index=True)\ndf = df.sample(frac=1.0, random_state=2024).reset_index(drop=True)\n\n# Export to csv\nprint(df.head())\nprint(df[\"label\"].value_counts())\ndf.to_csv(\"mvn_5d_A500_B500.csv\", index=False)\n</code></pre>"},{"location":"roteiro1/main/#visualize-the-data","title":"Visualize the Data","text":"<p>Since you cannot directly plot a 5D graph, you must reduce its dimensionality.</p> <ul> <li>Use a technique like Principal Component Analysis (PCA) to project the 5D data down to 2 dimensions.</li> <li>Create a scatter plot of this 2D representation, coloring the points by their class (A or B).</li> </ul> <pre><code>from sklearn.decomposition import PCA\n\nX = df[[\"f1\",\"f2\",\"f3\",\"f4\",\"f5\"]].values\ny = df[\"label\"].values\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nplt.figure(figsize=(7,5))\nfor label, color in zip([\"A\", \"B\"], [\"blue\", \"red\"]):\n    subset = X_pca[y == label]\n    plt.scatter(subset[:,0], subset[:,1], c=color, label=label, alpha=0.6)\n\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.title(\"PCA Projection of 5D Data to 2D\")\nplt.legend()\nplt.show()\n\nprint(\"Explained variance by PC1 and PC2:\", pca.explained_variance_ratio_)\n</code></pre> <p></p> <p>Scatter plot of points using PCA</p>"},{"location":"roteiro1/main/#analyze-the-plots","title":"Analyze the Plots","text":""},{"location":"roteiro1/main/#relationship-between-classes","title":"Relationship Between Classes","text":"<ul> <li> <p>In the 2D PCA scatter plot, Class A (blue) tends to cluster around the origin, while Class B (red) clusters around a shifted region (due to its mean vector [1.5,1.5,1.5,1.5,1.5]).</p> </li> <li> <p>However, because both classes share overlapping covariance structures, their projected clouds are not fully separated. So, there\u2019s an overlap region in the PCA space.</p> </li> <li> <p>This means:</p> </li> <li> <p>Many samples from A and B occupy similar regions, especially along dimensions where variance is high.</p> </li> <li> <p>The classes are distinguishable in aggregate but not with a simple, clean split.</p> </li> </ul>"},{"location":"roteiro1/main/#linear-separability","title":"Linear Separability","text":"<ul> <li>Linear separability means: you can draw a straight line (or, in higher dimensions, a hyperplane) that perfectly separates the two classes.</li> <li> <p>In the PCA 2D view, the overlap between clusters indicates that no straight line can perfectly separate them. This results in an impossibility for linear classifiers to create boundaries, as they assume a single straight boundary can separate classes</p> </li> <li> <p>Even in 5D, the covariance overlap means the classes are intertwined: a single hyperplane won\u2019t cleanly divide A and B.</p> </li> <li> <p>Neural networks with nonlinear activations (ReLU, tanh, sigmoid, etc.) can learn curved, flexible decision boundaries. That means:</p> </li> <li> <p>They don't restrict you to simple, linear boundaries. They can curve/warp the overlap to untangle structures</p> </li> <li> <p>Example:</p> <ul> <li> <p>A first hidden layer could capture correlated patterns (e.g., f1 &amp; f2 strongly linked).</p> </li> <li> <p>Later layers could combine features nonlinearly to carve out more complex decision regions.</p> </li> </ul> </li> <li> <p>This flexibility allows a neural network to outperform a simple linear classifier on data like this, where elliptical clouds overlap in multiple dimensions.</p> </li> </ul>"},{"location":"roteiro1/main/#excercise-3-preparing-real-world-data-for-a-neural-network","title":"Excercise 3 - Preparing Real-World Data for a Neural Network","text":""},{"location":"roteiro1/main/#get-the-data-download-the-spaceship-titanic-dataset-from-kaggle","title":"Get the Data: Download the Spaceship Titanic dataset from Kaggle","text":"<p>link to dataset : Spaceship Titanic</p>"},{"location":"roteiro1/main/#describing-the-data","title":"Describing the Data","text":"<p>Briefly describe the dataset's objective (i.e., what does the Transported column represent?).</p> <ul> <li>The dataset's objective is to identify if the passanger was transported to an alternate dimension or not</li> <li>The transported column is the target column which shows if the passanger was or not transported to an alternate dimension. It is a boolean classification of true or false</li> </ul> <p>List the features and identify which are numerical (e.g., Age, RoomService) and which are categorical (e.g., HomePlanet, Destination).</p> <ul> <li>PassengerId: Unique numerical code, identifier for each individual passenger</li> <li>Name: Categorical, name of passenger</li> <li>HomePlanet: Categorical, name of planet where passenger comes from</li> <li>CryoSleep: Boolean, shows if passenger was or not in CryoSleep</li> <li>Cabin: Structured string, can be split into categorical sub-features. shows cabin where passenger is hosted</li> <li>Destination: Categorical, destination of passenger</li> <li>Age: Numerical, age of passenger</li> <li>VIP: Boolean, shows if passenger is or not a VIP</li> <li>RoomService: Numerical, amount spent on room service</li> <li>FoodCourt: Numerical, amount spent on foodcourt</li> <li>ShoppingMall: Numerical, amount spent on shopping mall</li> <li>Spa: Numerical, amount spent on Spa</li> <li>VRDeck: Numerical, amount spent on VRDeck</li> <li>Transported: Boolean, shows if passenger was or not transported to alternate dimension</li> </ul> <p>Investigate the dataset for missing values. Which columns have them, and how many?</p> <ul> <li>HomePlanet      201</li> <li>CryoSleep       217</li> <li>Cabin           199</li> <li>Destination     182</li> <li>Age             179</li> <li>VIP             203</li> <li>RoomService     181</li> <li>FoodCourt       183</li> <li>ShoppingMall    208</li> <li>Spa             183</li> <li>VRDeck          188</li> <li>Name            200</li> </ul>"},{"location":"roteiro1/main/#preprocessing-the-data","title":"Preprocessing the Data","text":"<p>The goal is to clean and transform the data so it can be fed into a neural network. The tanh activation function produces outputs in the range [-1, 1], so the input data should be scaled appropriately for stable training.</p>"},{"location":"roteiro1/main/#handle-missing-data-devise-and-implement-a-strategy-to-handle-the-missing-values-in-all-the-affected-columns","title":"Handle Missing Data: Devise and implement a strategy to handle the missing values in all the affected columns","text":"<ul> <li>Median for age: avoids biases towards any particular demographic</li> <li>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck: para pre\u00e7os, em caso de falta, assumir valor 0</li> <li>HomePlanet, Destination: Fill as Unknown, as we have no information in regards to the origins and destinations</li> <li>CryoSleep, VIP: As there are more Non-VIP and Non-CryoSleep passengers, assume both values as False</li> <li>Cabin: Assume values as Unknown/0/U, ensuring cabins stay as unknown, impacting analysis the least as possible</li> </ul> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Load\ndf = pd.read_csv(\"spaceship/train.csv\")\n\n# Numerics\ndf[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())\nfor col in [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]:\n    df[col] = df[col].fillna(0)\n\n# Booleans\ndf[\"HomePlanet\"] = df[\"HomePlanet\"].fillna(\"Unknown\")\ndf[\"Destination\"] = df[\"Destination\"].fillna(\"Unknown\")\n# Ensure boolean dtype, then fill\ndf[\"CryoSleep\"] = df[\"CryoSleep\"].astype(\"boolean\").fillna(False)\ndf[\"VIP\"]       = df[\"VIP\"].astype(\"boolean\").fillna(False)\n\ndf[\"Cabin\"] = df[\"Cabin\"].fillna(\"Unknown/0/U\")\ncparts = df[\"Cabin\"].str.split(\"/\", expand=True)\ncparts.columns = [\"Deck\", \"CabinNum\", \"Side\"]\ndf = pd.concat([df.drop(columns=[\"Cabin\"]), cparts], axis=1)\n\ndf[\"CabinNum\"] = pd.to_numeric(df[\"CabinNum\"], errors=\"coerce\").fillna(0)\n\npid = df[\"PassengerId\"].str.split(\"_\", expand=True)\npid.columns = [\"GroupId\", \"GroupIdx\"]\ndf[\"GroupId\"] = pd.to_numeric(pid[\"GroupId\"], errors=\"coerce\").fillna(0)\ndf[\"GroupIdx\"] = pd.to_numeric(pid[\"GroupIdx\"], errors=\"coerce\").fillna(0)\n\ndf = df.drop(columns=[\"Name\", \"PassengerId\"])\n\ncat_cols = [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"]\n\ndf[\"CryoSleep\"] = df[\"CryoSleep\"].astype(\"int8\")\ndf[\"VIP\"]       = df[\"VIP\"].astype(\"int8\")\n\ndf = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n\n# Scale numerics to [-1, 1] for tanh\nnum_cols = [\n    \"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\",\n    \"CabinNum\", \"GroupId\", \"GroupIdx\"\n]\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\ndf[num_cols] = scaler.fit_transform(df[num_cols])\n\n# Map binary 0/1 columns to -1/1 (helps symmetry for tanh)\nfor bcol in [\"CryoSleep\", \"VIP\"]:\n    df[bcol] = df[bcol].map({0: -1.0, 1: 1.0})\n\nfor col in df.columns:\n    if df[col].dtype == \"bool\":\n        df[col] = df[col].astype(\"float32\")\n    elif pd.api.types.is_integer_dtype(df[col]) or pd.api.types.is_float_dtype(df[col]):\n        df[col] = df[col].astype(\"float32\")\n\nprint(df.shape, \"columns:\", len(df.columns))\nprint(df.head(3))\n</code></pre>"},{"location":"roteiro1/main/#encode-categorical-features","title":"Encode Categorical Features","text":"<p>Convert categorical columns like HomePlanet, CryoSleep, and Destination into a numerical format. For this, we will use one-hot encoding.</p> <p>Normalize Numerical Features: Scale the numerical columns (e.g., Age, RoomService, etc.). Since the tanh activation function is centered at zero and outputs values in [-1, 1], Normalization to a [-1, 1] range are excellent choices. It is a good idea to do so because:</p> <ul> <li> <p>Different features may live on very different scales (Age ~ 0\u201380, RoomService ~ 0\u201310,000).</p> </li> <li> <p>Features with large magnitudes dominate the loss surface, while smaller ones contribute very little. As such, an imbalance is presented in learning.</p> </li> <li> <p>Normalization has ranges between [-1, 1]. It\u2019s steepest around 0 and flattens near the extremes.</p> </li> <li> <p>It is good practice because:</p> </li> <li> <p>Keeps all features comparable: avoids features dominating over others.</p> </li> <li> <p>Matches tanh\u2019s natural range: inputs in [-1,1] align perfectly with the output scale.</p> </li> <li> <p>Faster convergence: optimizers like SGD, Adam work better with well-scaled features.</p> </li> <li> <p>Improves generalization: network learns smoother decision boundaries.</p> </li> </ul>"},{"location":"roteiro1/main/#visualize-data","title":"Visualize data","text":"<p>With those steps done, we will create histograms for two numerical features ,FoodCourt and Age, before and after scaling to show the effect of the transformation.</p> <pre><code>from sklearn.preprocessing import MinMaxScaler\n\n# Load dataset (adjust path if needed)\ndf_raw = pd.read_csv(\"spaceship/train.csv\")\n\n# Pick numerical features\nnum_cols = [\"Age\", \"FoodCourt\"]\n\n# Handle missing values\ndf_raw[\"Age\"] = df_raw[\"Age\"].fillna(df_raw[\"Age\"].median())\ndf_raw[\"FoodCourt\"] = df_raw[\"FoodCourt\"].fillna(0)\n\n# Scale with MinMaxScaler to [-1, 1]\nscaler = MinMaxScaler(feature_range=(-1, 1))\ndf_scaled = df_raw.copy()\ndf_scaled[num_cols] = scaler.fit_transform(df_scaled[num_cols])\n\n# Plot histograms before and after scaling\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n# Age before and after\naxes[0,0].hist(df_raw[\"Age\"], bins=30, color=\"skyblue\", edgecolor=\"black\")\naxes[0,0].set_title(\"Age - Before Scaling\")\naxes[0,1].hist(df_scaled[\"Age\"], bins=30, color=\"lightgreen\", edgecolor=\"black\")\naxes[0,1].set_title(\"Age - After Scaling\")\n\n# FoodCourt before and after\naxes[1,0].hist(df_raw[\"FoodCourt\"], bins=30, color=\"skyblue\", edgecolor=\"black\")\naxes[1,0].set_title(\"FoodCourt - Before Scaling\")\naxes[1,1].hist(df_scaled[\"FoodCourt\"], bins=30, color=\"lightgreen\", edgecolor=\"black\")\naxes[1,1].set_title(\"FoodCourt - After Scaling\")\n\nfor ax in axes.flat:\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>The output is as below:</p> <p></p> <p>Histograms of Age and Foodcourt</p>"},{"location":"roteiro2/main/","title":"Roteiro 2","text":""},{"location":"roteiro2/main/#2-perceptron-understanding-perceptrons-and-their-limitations","title":"2 - Perceptron: Understanding Perceptrons and Their Limitations","text":"<p>This activity is designed to test skills in Perceptrons and their limitations.</p>"},{"location":"roteiro2/main/#exercise-1","title":"Exercise 1","text":"<p>Start by importing necessary libraries for this project:</p> pip install matplotlib pandas numpy"},{"location":"roteiro2/main/#data-generation-task","title":"Data Generation Task","text":"<p>Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li>Class 0:</li> <li> <p>Mean = [2,2]</p> </li> <li> <p>Covariance matrix =     [ \\begin{bmatrix} 0.5 &amp; 0 \\ 0 &amp; 0.5 \\end{bmatrix} ]</p> </li> <li> <p>Class 1:</p> </li> <li> <p>Mean = [5,5]</p> </li> <li> <p>Covariance matrix =     [ \\begin{bmatrix} 0.5 &amp; 0 \\ 0 &amp; 0.5 \\end{bmatrix} ]</p> </li> </ul> <pre><code># Reproducibility\nseed = 120901\nrng = np.random.default_rng(seed)\n\n\nmean0 = np.array([2.0, 2.0])\nmean1 = np.array([5.0, 5.0])\ncov = np.array([[0.5, 0.0],\n                [0.0, 0.5]])\n\nn_per_class = 1000\n\n# Output names\nout_prefix = \"two_class_data\"\ncsv_path = f\"{out_prefix}.csv\"\nparquet_path = f\"{out_prefix}.parquet\"\n\n# Generate samples\nX0 = rng.multivariate_normal(mean0, cov, size=n_per_class)\nX1 = rng.multivariate_normal(mean1, cov, size=n_per_class)\n\n# To DataFrames\ndf0 = pd.DataFrame(X0, columns=[\"x\", \"y\"])\ndf0[\"class\"] = 0\n\ndf1 = pd.DataFrame(X1, columns=[\"x\", \"y\"])\ndf1[\"class\"] = 1\n\n# Combined dataset\ndf = pd.concat([df0, df1], ignore_index=True)\ndf.head()\n\nX = df[[\"x\", \"y\"]].to_numpy(dtype=float)\ny = df[\"class\"].to_numpy(dtype=int)\n</code></pre> <p>These parameters ensure the classes are mostly linearly separable, with minimal overlap due to the distance between means and low variance. Plot the data points (using libraries like matplotlib if desired) to visualize the separation, coloring points by class.</p> <pre><code>plt.figure(figsize=(5, 5))\nmask0 = df[\"class\"] == 0\nmask1 = df[\"class\"] == 1\n\nplt.scatter(df.loc[mask0, \"x\"], df.loc[mask0, \"y\"], s=10, label=\"Class 0\")\nplt.scatter(df.loc[mask1, \"x\"], df.loc[mask1, \"y\"], s=10, label=\"Class 1\")\n\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Two Classes from 2D Gaussians\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Plot of classes</p>"},{"location":"roteiro2/main/#perceptron-implementation-task","title":"Perceptron Implementation Task","text":"<p>Implement a single-layer perceptron from scratch to classify the generated data into the two classes. You may use NumPy only for basic linear algebra operations (e.g., matrix multiplication, vector addition/subtraction, dot products). Do not use any pre-built machine learning libraries (e.g., no scikit-learn) or NumPy functions that directly implement perceptron logic.</p> <ul> <li>Initialize weights (w) as a 2D vector (plus a bias term b).</li> </ul> <pre><code>y_bin = np.where(y == 1, 1, -1)\n\nw = np.zeros(2, dtype=float)\nb = 0.0\nn = 0.01\n\nw, b, n\n</code></pre> <ul> <li>Use the perceptron learning rule</li> </ul> <pre><code>def perceptron_predict_bin(X, w, b):\n    a = X @ w + b\n    return np.where(a &gt;= 0, 1, -1)  # returns \u00b11\n\ndef accuracy_bin(y_true_bin, y_pred_bin):\n    return float((y_true_bin == y_pred_bin).mean())\n</code></pre> <ul> <li>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point. Track accuracy after each epoch.</li> </ul> <pre><code>def perceptron_train_until_converged(\n    X, y_bin, w, b, n=0.01, max_epochs=100, shuffle=True, seed=120901\n):\n    N = X.shape[0]\n    rng = np.random.default_rng(seed)\n\n    acc_history = []\n    mistakes_history = []\n    epochs_run = 0\n    converged = False\n\n    for epoch in range(1, max_epochs + 1):\n        if shuffle:\n            idx = np.arange(N)\n            rng.shuffle(idx)\n            Xe, ye = X[idx], y_bin[idx]\n        else:\n            Xe, ye = X, y_bin\n\n        mistakes = 0\n\n        for xi, yi in zip(Xe, ye):\n            a = float(np.dot(w, xi) + b)\n            if yi * a &lt;= 0:\n                w = w + n * yi * xi\n                b = b + n * yi\n                mistakes += 1\n\n        # After completing this epoch, compute accuracy on the full dataset\n        y_hat = perceptron_predict_bin(X, w, b)\n        acc = accuracy_bin(y_bin, y_hat)\n        acc_history.append(acc)\n        mistakes_history.append(mistakes)\n\n        epochs_run = epoch\n        if mistakes == 0:\n            converged = True\n            break\n\n    return {\n        \"w\": w,\n        \"b\": b,\n        \"acc_history\": acc_history,\n        \"mistakes_history\": mistakes_history,\n        \"epochs_run\": epochs_run,\n        \"converged\": converged,\n    }\n\nresult = perceptron_train_until_converged(X, y_bin, w, b, n=n, max_epochs=100, shuffle=True, seed=seed)\nresult[\"converged\"], result[\"epochs_run\"], result[\"acc_history\"][:5], result[\"mistakes_history\"][:5]\n</code></pre> <pre><code>(False, 100, [0.9995, 0.9995, 0.9995, 0.9995, 0.999], [54, 9, 20, 17, 26])\n</code></pre> <ul> <li>After training, evaluate accuracy on the full dataset and plot the decision boundary overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress. Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.</li> </ul> <pre><code># Use the trained parameters\nw_tr, b_tr = result[\"w\"], result[\"b\"]\n\n# Scatter the data\nmask0 = (y_bin == -1)\nmask1 = (y_bin == 1)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(X[mask0, 0], X[mask0, 1], s=10, label=\"Class 0 (-1)\")\nplt.scatter(X[mask1, 0], X[mask1, 1], s=10, label=\"Class 1 (+1)\")\n\n# Plot decision boundary\nx_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\ny_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\neps = 1e-12\n\nif abs(w_tr[1]) &gt; eps:\n    xs = np.linspace(x_min, x_max, 400)\n    ys = -(w_tr[0] * xs + b_tr) / w_tr[1]\n    plt.plot(xs, ys, linewidth=2, label=\"Perceptron boundary\")\nelif abs(w_tr[0]) &gt; eps:\n    # Vertical boundary\n    x0 = -b_tr / w_tr[0]\n    plt.axvline(x0, linewidth=2, label=\"Perceptron boundary\")\nelse:\n    print(\"Weights near zero; boundary not defined yet.\")\n\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Perceptron decision boundary (n = 0.01)\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Plot of perceptron decision boundary</p> <p>The final weights, bias, and accuracy are as follows:</p> <ul> <li>weight: [0.12208697 0.15098059]</li> <li>bias: -0.9300000000000006</li> <li>accuracy: 0.9995</li> </ul> <p>Besides that, here is the plot of the Perceptrons accuracy per epoch:</p> <p></p> <p>Plot of Perceptron Accuracy per epoch</p> <p>The convergence is easier as the points being distributed has low overlap between classes. As such, finding a decision boundary is made simple, even in low epoch counts.</p>"},{"location":"roteiro2/main/#excercise-2","title":"Excercise 2","text":""},{"location":"roteiro2/main/#data-generation-task-2","title":"Data Generation Task 2","text":"<p>Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li>Class 0:</li> <li> <p>Mean = [3,3]</p> </li> <li> <p>Covariance matrix =   [ \\begin{bmatrix} 1.5 &amp; 0 \\ 0 &amp; 1.5 \\end{bmatrix} ]</p> </li> <li> <p>Class 1:</p> </li> <li> <p>Mean = [4,4]</p> </li> <li> <p>Covariance matrix   [ \\begin{bmatrix} 1.5 &amp; 0 \\ 0 &amp; 1.5 \\end{bmatrix} ]</p> </li> </ul> <p></p> <p>Plot of Classes</p> <p>These parameters create partial overlap between classes due to closer means and higher variance, making the data not fully linearly separable.</p>"},{"location":"roteiro2/main/#perceptron-implementation-task-2","title":"Perceptron Implementation Task 2","text":"<p>Using the same implementation guidelines as in Exercise 1, we will train a perceptron on this dataset.</p> <ul> <li>Follow the same initialization, update rule, and training process.</li> <li>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point and note any oscillation in updates; consider reporting the best accuracy achieved over multiple runs (e.g., average over 5 random initializations). Track accuracy after each epoch.</li> <li>Evaluate accuracy after training and plot the decision boundary overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress (or lack thereof). Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.</li> </ul> <p></p> <p>Plot of perceptron decision boundary</p> <ul> <li>As seen above, the decision boundary is not able to separate the data points in a linear fashion. As such, the classes aren't properly separated.</li> </ul> <p></p> <p>Plot of Perceptron accuracy per epoch</p> <ul> <li>Perceptrons accuracy, comparing to it's accuracy in excercise 1, is not as accurate and oscilates between accuracy levels. Overall, the classifying model is less effective here.</li> </ul> <p>The final weights, bias, and accuracy are as follows:</p> <ul> <li>weight: [0.06873479 0.20793337]</li> <li>bias: -0.9200000000000006</li> <li>accuracy: 0.7735</li> </ul> <p>As we can see, convergence is not possible in this dataset and 100% accuracy is also impossible.</p>"},{"location":"roteiro3/main/","title":"Roteiro 3","text":""},{"location":"roteiro3/main/#3-mlp-understanding-multi-layer-perceptrons-mlps","title":"3 - MLP: Understanding Multi-Layer Perceptrons (MLPs)","text":"<p>This activity is designed to test our skills in Multi-Layer Perceptrons (MLPs).</p>"},{"location":"roteiro3/main/#exercise-1-manual-calculation-of-mlp-steps","title":"Exercise 1: Manual Calculation of MLP Steps","text":"<p>Consider a simple MLP with 2 input features, 1 hidden layer containing 2 neurons, and 1 output neuron. Use the hyperbolic tangent (tanh) function as the activation for both the hidden layer and the output layer. The loss function is mean squared error (MSE): L=1/N(y-^y)\u00b2, where ^y is the networks output.</p> <p>use the following specific values:</p> <ul> <li>Input and output vectors:</li> <li>x = [0.5, -0.2]</li> <li> <p>y = 1.0</p> </li> <li> <p>Hidden layer weights:</p> </li> <li> <p>W1 = [ [0.3, -0.1] , [0.2, 0.4] ]</p> </li> <li> <p>Hidden layer biases:</p> </li> <li> <p>b1 = [0.1 , -0.2]</p> </li> <li> <p>Output layer weights:</p> </li> <li> <p>W2 = [0.5 , -0.3]</p> </li> <li> <p>Output layer bias:</p> </li> <li> <p>b2 = 0.2</p> </li> <li> <p>Learning rate:</p> </li> <li> <p>n = 0.3</p> </li> <li> <p>Activation function:</p> </li> <li>tanh</li> </ul> <p>Perform the following steps explicitly, showing all mathematical derivations and calculations with the provided values:</p>"},{"location":"roteiro3/main/#forward-pass","title":"Forward Pass","text":"<ul> <li>Compute the hidden layer pre-activations</li> <li>Apply tanh to get hidden activations</li> <li>Compute the output pre-activation</li> <li>Compute the final output</li> </ul>"},{"location":"roteiro3/main/#loss-calculation","title":"Loss Calculation","text":"<ul> <li>Compute the MSE loss</li> </ul>"},{"location":"roteiro3/main/#backward-pass-backpropagation","title":"Backward Pass (Backpropagation)","text":"<p>Compute the gradients of the loss with respect to all weights and biases. Compute:</p> <ul> <li>Using tanh derivative</li> <li>Gradients for output layer</li> <li>Propagate to hidden layer</li> <li>Gradients for hidden layer</li> </ul> <p>Show all intermediate steps and calculations.</p>"},{"location":"roteiro3/main/#parameter-update","title":"Parameter Update","text":"<p>Using the learning rate n = 0.1, update all weights and biases via gradient descent</p> <p>Show all mathematical steps explicitly, including intermediate calculations (e.g., matrix multiplications, tanh applications, gradient derivations). Use exact numerical values throughout and avoid rounding excessively to maintain precision (at least 4 decimal places).</p>"},{"location":"roteiro3/main/#exercise-2-binary-classification-with-synthetic-data-and-scratch-mlp","title":"Exercise 2: Binary Classification with Synthetic Data and Scratch MLP","text":"<p>Using the make_classification function from scikit-learn, generate a synthetic dataset with the following specifications:</p> <ul> <li>Number of Samples: 1000</li> <li>Number of Classes: 2</li> <li>Number of clusters per class: Use the n_clusters_per_class parameter creatively to achieve 1 cluster for one class and 2 for the other (hint: you may need to generate subsets separately and combine them, as the function applies the same number of clusters to all classes by default).</li> <li>Other parameters: Set n_features=2 for easy visualization, n_informative=2, n_redundant=0, random_state=42 for reproducibility, and adjust class_sep or flip_y as needed for a challenging but separable dataset.</li> </ul> <p>Implement an MLP from scratch (without using libraries like TensorFlow or PyTorch for the model itself; you may use NumPy for array operations) to classify this data. You have full freedom to choose the architecture, including:</p> <ul> <li>Number of hidden layers (at least 1)</li> <li>Number of neurons per layer</li> <li>Activation functions (e.g., sigmoid, ReLU, tanh)</li> <li>Loss function (e.g., binary cross-entropy)</li> <li>Optimizer (e.g., gradient descent, with a chosen learning rate)</li> </ul> <p>Steps to follow:</p> <ul> <li>Generate and split the data into training (80%) and testing (20%) sets.</li> <li>Implement the forward pass, loss computation, backward pass, and parameter updates in code.</li> <li>Train the model for a reasonable number of epochs (e.g., 100-500), tracking training loss.</li> <li>Evaluate on the test set: Report accuracy, and optionally plot decision boundaries or confusion matrix.</li> <li>Submit your code and results, including any visualizations.</li> </ul>"},{"location":"roteiro3/main/#exercise-3-multi-class-classification-with-synthetic-data-and-reusable-mlp","title":"Exercise 3: Multi-Class Classification with Synthetic Data and Reusable MLP","text":"<p>Use make_classification to generate a synthetic dataset with:</p> <ul> <li>Number of samples: 1500</li> <li>Number of classes: 3</li> <li>Number of features: 4</li> <li>Number of clusters per class: Achieve 2 clusters for one class, 3 for another, and 4 for the last (again, you may need to generate subsets separately and combine them, as the function doesn't directly support varying clusters per class).</li> <li>Other parameters: n_features=4, n_informative=4, n_redundant=0, random_state=42.</li> </ul> <p>Implement an MLP from scratch to classify this data. You may choose the architecture freely, but for an extra point (bringing this exercise to 4 points), reuse the exact same MLP implementation code from Exercise 2, modifying only hyperparameters (e.g., output layer size for 3 classes, loss function to categorical cross-entropy if needed) without changing the core structure.</p> <p>Steps:</p> <ul> <li>Generate and split the data (80/20 train/test).</li> <li>Train the model, tracking loss.</li> <li>Evaluate on test set: Report accuracy, and optionally visualize (e.g., scatter plot of data with predicted labels).</li> <li>Submit code and results.</li> </ul>"},{"location":"roteiro3/main/#exercise-4-multi-class-classification-with-deeper-mlp","title":"Exercise 4: Multi-Class Classification with Deeper MLP","text":"<p>Repeat Exercise 3 exactly, but now ensure your MLP has at least 2 hidden layers. You may adjust the number of neurons per layer as needed for better performance. Reuse code from Exercise 3 where possible, but the focus is on demonstrating the deeper architecture. Submit updated code, training results, and test evaluation.</p>"},{"location":"roteiro4/main/","title":"Roteiro 4","text":""},{"location":"roteiro4/main/#se-chegou-aqui-e-porque-voce-esta-interessado-em-saber-mais-logo-de-brinde-como-rodar-um-codigo-python-aqui","title":"Se chegou aqui, \u00e9 porque voc\u00ea est\u00e1 interessado em saber mais. Logo, de brinde, como rodar um c\u00f3digo <code>Python</code> aqui","text":"2025-09-08T16:24:34.992845 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ 2025-09-08T16:24:36.401088 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p>Markdown-exec \u00e9 uma extens\u00e3o do Markdown que permite executar c\u00f3digo Python diretamente no Markdown. Isso \u00e9 \u00fatil para gerar resultados din\u00e2micos ou executar scripts de forma interativa.</p>"},{"location":"roteiro4/main/#roteiro-1-data-preparation-and-analysis-for-neural-networks","title":"Roteiro 1 - Data Preparation and Analysis for Neural Networks","text":"<p>This activity is designed to test your skills in generating synthetic datasets, handling real-world data challenges, and preparing data to be fed into neural networks.</p>"},{"location":"roteiro4/main/#excercise-1-exploring-class-separability-in-2d","title":"Excercise 1 - Exploring Class Separability in 2D","text":"<p>generate and visualize a two-dimensional dataset to explore how data distribution affects the complexity of the decision boundaries a neural network would need to learn.</p>"},{"location":"roteiro4/main/#generate-the-data","title":"Generate the Data","text":"<p>Start by importing necessary libraries for this project:</p> pip install matplotlib pandas scikit-learn numpy <p>Create a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each). Use a Gaussian distribution to generate the points for each class based on the following parameters:</p> <ul> <li>Class 0: Mean = [2,3] , Standard Deviation = [0.8,2.5]</li> <li>Class 1: Mean = [5,6], Standard Deviation = [1.2,1.9]</li> <li>Class 2: Mean = [8,1], Standard Deviation = [0.9,0.9]</li> <li>Class 3: Mean = [15,4], Standard Deviation = [0.5,2.0]</li> </ul> <p> </p> Editor (session: default) Run <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(42)\n\nparams = {\n    0: {\"mean\": np.array([2.0, 3.0]), \"std\": np.array([0.8, 2.5])},\n    1: {\"mean\": np.array([5.0, 6.0]), \"std\": np.array([1.2, 1.9])},\n    2: {\"mean\": np.array([8.0, 1.0] ), \"std\": np.array([0.9, 0.9])},\n    3: {\"mean\": np.array([15.0, 4.0]), \"std\": np.array([0.5, 2.0])},\n}\n\nn_samples_class = 100\n\ndata_list = []\nfor cls, p in params.items():\n    samples = rng.normal(loc=p[\"mean\"], scale=p[\"std\"], size=(n_samples_class, 2))\n    labels = np.full((n_samples_class, 1), cls, dtype=int)\n    data_list.append(np.hstack([samples, labels]))\n\n# Combine\ndata = np.vstack(data_list)\ndf = pd.DataFrame(data, columns=[\"x1\", \"x2\", \"label\"]).astype({\"label\": int})\n\n# Shuffle rows\ndf = df.sample(frac=1.0, random_state=123).reset_index(drop=True)\n\n# Save to CSV\ncsv_path = \"synthetic_gaussian_4class_400.csv\"\ndf.to_csv(csv_path, index=False)\n\n# Show class counts\ncounts = df[\"label\"].value_counts().sort_index()\n\n# Display a preview to the user\ndf.head(500)\n</pre> Output Clear <pre></pre> <p></p> <p></p> <p>Dashboard do MAAS</p> <p>Conforme ilustrado acima, a tela inicial do MAAS apresenta um dashboard com informa\u00e7\u00f5es sobre o estado atual dos servidores gerenciados. O dashboard \u00e9 composto por diversos pain\u00e9is, cada um exibindo informa\u00e7\u00f5es sobre um aspecto espec\u00edfico do ambiente gerenciado. Os pain\u00e9is podem ser configurados e personalizados de acordo com as necessidades do usu\u00e1rio.</p>"},{"location":"roteiro4/main/#tarefa-2","title":"Tarefa 2","text":""},{"location":"roteiro4/main/#app","title":"App","text":""},{"location":"roteiro4/main/#tarefa-1","title":"Tarefa 1","text":"<p>Exemplo de diagrama</p> <pre><code>architecture-beta\n    group api(cloud)[API]\n\n    service db(database)[Database] in api\n    service disk1(disk)[Storage] in api\n    service disk2(disk)[Storage] in api\n    service server(server)[Server] in api\n\n    db:L -- R:server\n    disk1:T -- B:server\n    disk2:T -- B:db</code></pre> <p>Mermaid</p>"},{"location":"roteiro4/main/#questionario-projeto-ou-plano","title":"Question\u00e1rio, Projeto ou Plano","text":"<p>Esse se\u00e7\u00e3o deve ser preenchida apenas se houver demanda do roteiro.</p>"},{"location":"roteiro4/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Quais as dificuldades encontradas? O que foi mais f\u00e1cil? O que foi mais dif\u00edcil?</p>"},{"location":"roteiro4/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O que foi poss\u00edvel concluir com a realiza\u00e7\u00e3o do roteiro?</p>"},{"location":"roteiro4/main/#diagrama-de-classes-do-banco","title":"Diagrama de Classes do Banco","text":"<pre><code>classDiagram\n    class Conta {\n        - String id\n        # double saldo\n        - Cliente cliente\n        + sacar(double valor)\n        + depositar(double valor)\n    }\n    class Cliente {\n        - String id\n        - String nome\n        - List&lt;Conta&gt; contas\n    }\n    class PessoaFisica {\n        - String cpf\n    }\n    class PessoaJuridica {\n        - String cnpj\n    }\n    class ContaCorrente {\n        - double limite\n        + sacar(double valor)\n    }\n    class ContaPoupanca {\n        + sacar(double valor)\n    }\n    Conta *-- Cliente\n    Conta &lt;|-- ContaCorrente\n    Conta &lt;|-- ContaPoupanca\n    Cliente &lt;|-- PessoaFisica\n    Cliente &lt;|-- PessoaJuridica</code></pre>"},{"location":"roteiro4/main/#diagrama-de-sequencia-de-autorizacao","title":"Diagrama de Seq\u00fc\u00eancia de Autoriza\u00e7\u00e3o","text":"<pre><code>sequenceDiagram\n  autonumber\n  actor User\n  User-&gt;&gt;Auth Service: request with token\n  Auth Service-&gt;&gt;Auth Service: decodes the token and extracts claims\n  Auth Service-&gt;&gt;Auth Service: verifies permissions\n  critical allowed\n    Auth Service-&gt;&gt;Secured Resource: authorizes the request\n    Secured Resource-&gt;&gt;User: returns the response\n  option denied\n    Auth Service--&gt;&gt;User: unauthorized message\n  end  </code></pre> <p>Running the code below in Browser (Woooooowwwwww!!!!!!). <sup>1</sup></p> <p> </p> Editor (session: default) Run <pre>import ssl\nimport pandas as pd\n\ndf = pd.DataFrame()\ndf['AAPL'] = pd.Series([1, 2, 3])\ndf['MSFT'] = pd.Series([4, 5, 6])\ndf['GOOGL'] = pd.Series([7, 8, 9])\n\nprint(df)\n</pre> Output Clear <pre><code></code></pre> <p></p> <ol> <li> <p>Pyodide \u21a9</p> </li> </ol>"}]}